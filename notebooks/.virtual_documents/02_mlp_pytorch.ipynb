





import torch
from torch import nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


train = pd.read_csv("../data/raw/train.csv")
test = pd.read_csv("../data/raw/test.csv")

print(train.shape)
print(test.shape)


all_features = pd.concat((train.iloc[:, 1:-1], test.iloc[:, 1:]))





numberic_features = all_features.dtypes[all_features.dtypes != 'object'].index

all_features[numberic_features] = all_features[numberic_features].apply(
    lambda x:(x-x.mean())/x.std())
all_features[numberic_features] = all_features[numberic_features].fillna(0)





all_features = pd.get_dummies(all_features, dummy_na=True)
all_features.shape





all_features = all_features.astype(float)
n_train = train.shape[0]

train_features = torch.tensor(
    all_features[:n_train].values,
    dtype=torch.float32
)

test_features = torch.tensor(
    all_features[n_train:].values,
    dtype=torch.float32
)

train_labels = torch.tensor(
    train.SalePrice.values.reshape(-1,1),
    dtype=torch.float32
)


all_features.dtypes.value_counts()
train_features.shape,test_features.shape





train_labels = torch.log1p(train_labels)








in_feature = train_features.shape[1]

net = nn.Sequential(nn.Linear(in_feature,1))





loss = nn.MSELoss()

optimizer = torch.optim.Adam(
    net.parameters(),
    lr=0.01,
    weight_decay=0 #先观察未使用L2正则时
)





def train(net,train_features,train_labels,
         num_epochs=100,lr=0.01,weight_decay=0):
    
    optimizer = torch.optim.Adam(
        net.parameters(),
        lr=lr,
        weight_decay=weight_decay
    )
    train_ls = []

    for epoch in range(num_epochs):
        net.train()
        optimizer.zero_grad()
        l = loss(net(train_features),train_labels)
        l.backward()
        optimizer.step()

        train_ls.append(l.item())

        if (epoch + 1)%20 == 0:
            print(f"epoch:{epoch+1},loss:{l.item():.6f}")
    return train_ls


train_ls = train(
    net,
    train_features,
    train_labels,
    num_epochs=200,
    lr=0.01
)


plt.plot(train_ls)
plt.xlabel("epoch")
plt.ylabel("MSE loss")
plt.title("Training Loss")
plt.show()











net = nn.Sequential(
    nn.Linear(in_feature,128),
    nn.ReLU(),
    nn.Linear(128,1)
)





def train(net, train_features, train_labels,
          num_epochs=200, lr=0.01, weight_decay=1e-3):

    optimizer = torch.optim.Adam(
        net.parameters(),
        lr=lr,
        weight_decay=weight_decay
    )
    
    train_ls = []

    for epoch in range(num_epochs):
        net.train()
        
        optimizer.zero_grad()
        l = loss(net(train_features), train_labels)
        l.backward()
        optimizer.step()
        
        train_ls.append(l.item())

    return train_ls


train_ls = train(
    net,
    train_features,
    train_labels,
    num_epochs=300,
    lr=0.01,
    weight_decay=1e-3
)


plt.plot(train_ls)
plt.title("2-layer MLP with L2")
plt.show()








def get_k_fold_data(k,i,X,y):
    fold_size = X.shape[0] // k
    X_train,y_train = None,None
    X_valid,y__valid = None,None

    for j in range(fold_size):
        idx = slice(j*fold_size,(j+1)*fold_size)
        X_part,y_part = X[idx,:],y[idx]
        if j == i:
            X_valid,y__valid = X_part,y_part
        else:
            if X_train == None:
                X_train,y_train = X_part,y_part
            else:
                X_train = torch.cat([X_train, X_part], 0)
                y_train = torch.cat([y_train, y_part], 0)
    return X_train,y_train,X_valid,y__valid


def k_fold(k, X, y, num_epochs=200, lr=0.01, weight_decay=1e-3):
    train_l_sum, valid_l_sum = 0, 0

    for i in range(k):
        net = nn.Sequential(
            nn.Linear(in_feature, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        X_train, y_train, X_valid, y_valid = \
            get_k_fold_data(k, i, X, y)
        
        train_ls = train(
            net, X_train, y_train,
            num_epochs, lr, weight_decay
        )
        
        valid_loss = loss(net(X_valid), y_valid).item()
        
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_loss
        
        print(f"fold {i}, valid loss {valid_loss:.6f}")
     
    return train_l_sum / k, valid_l_sum / k


train_loss, valid_loss = k_fold(
    5,
    train_features,
    train_labels,
    num_epochs=200,
    lr=0.01,
    weight_decay=1e-3
)

print(f"avg train loss: {train_loss:.5f}")
print(f"avg valid loss: {valid_loss:.5f}")





def k_fold(k, X, y, num_epochs=200, lr=0.01, weight_decay=1e-3):
    train_l_sum, valid_l_sum = 0, 0

    for i in range(k):
        net = nn.Sequential(
            nn.Linear(in_feature, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 1)
        )
        X_train, y_train, X_valid, y_valid = \
            get_k_fold_data(k, i, X, y)
        
        train_ls = train(
            net, X_train, y_train,
            num_epochs, lr, weight_decay
        )
        
        valid_loss = loss(net(X_valid), y_valid).item()
        
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_loss
        
        print(f"fold {i}, valid loss {valid_loss:.6f}")
     
    return train_l_sum / k, valid_l_sum / k


train_loss, valid_loss = k_fold(
    5,
    train_features,
    train_labels,
    num_epochs=200,
    lr=0.01,
    weight_decay=1e-3
)

print(f"avg train loss: {train_loss:.5f}")
print(f"avg valid loss: {valid_loss:.5f}")





def k_fold(k, X, y, num_epochs=200, lr=0.01, weight_decay=1e-3):
    train_l_sum, valid_l_sum = 0, 0

    for i in range(k):
        net = nn.Sequential(
            nn.Linear(in_feature, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        X_train, y_train, X_valid, y_valid = \
            get_k_fold_data(k, i, X, y)
        
        train_ls = train(
            net, X_train, y_train,
            num_epochs, lr, weight_decay
        )
        
        valid_loss = loss(net(X_valid), y_valid).item()
        
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_loss
        
        print(f"fold {i}, valid loss {valid_loss:.6f}")
     
    return train_l_sum / k, valid_l_sum / k


train_loss, valid_loss = k_fold(
    5,
    train_features,
    train_labels,
    num_epochs=200,
    lr=0.01,
    weight_decay=1e-3
)

print(f"avg train loss: {train_loss:.5f}")
print(f"avg valid loss: {valid_loss:.5f}")





def k_fold(k, X, y, num_epochs=200, lr=0.01, weight_decay=1e-3):
    train_l_sum, valid_l_sum = 0, 0

    for i in range(k):
        net = nn.Sequential(
            nn.Linear(in_feature, 1),
        )
        X_train, y_train, X_valid, y_valid = \
            get_k_fold_data(k, i, X, y)
        
        train_ls = train(
            net, X_train, y_train,
            num_epochs, lr, weight_decay
        )
        
        valid_loss = loss(net(X_valid), y_valid).item()
        
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_loss
        
        print(f"fold {i}, valid loss {valid_loss:.6f}")
     
    return train_l_sum / k, valid_l_sum / k


train_loss, valid_loss = k_fold(
    5,
    train_features,
    train_labels,
    num_epochs=200,
    lr=0.01,
    weight_decay=0
)

print(f"avg train loss: {train_loss:.5f}")
print(f"avg valid loss: {valid_loss:.5f}")



